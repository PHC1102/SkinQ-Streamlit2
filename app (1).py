import streamlit as st
import requests
import json
import base64
from PIL import Image
import io
import torch
from transformers import AutoImageProcessor, AutoModelForImageClassification
import numpy as np

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Medical Chatbot",
    page_icon="ü©∫",
    layout="wide"
)

# L·∫•y API key t·ª´ secrets
OPENROUTER_API_KEY = st.secrets["OPENROUTER_API_KEY"]

# URLs API
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

@st.cache_resource
def load_skin_disease_model():
    """Load dinov2-skindisease-finetuned model locally"""
    try:
        processor = AutoImageProcessor.from_pretrained("dinov2-skindisease-finetuned")
        model = AutoModelForImageClassification.from_pretrained("dinov2-skindisease-finetuned")
        return processor, model
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ load model: {str(e)}")
        return None, None

def call_gpt_oss(messages):
    """G·ªçi GPT-OSS qua OpenRouter"""
    headers = {
        "Authorization": f"Bearer {OPENROUTER_API_KEY}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": "gpt-oss",
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 1000
    }
    
    try:
        response = requests.post(OPENROUTER_URL, headers=headers, json=data)
        response.raise_for_status()
        return response.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"L·ªói khi g·ªçi API: {str(e)}"

def analyze_skin_image(image, processor, model):
    """Ph√¢n t√≠ch ·∫£nh da li·ªÖu b·∫±ng model local"""
    try:
        # Preprocess ·∫£nh
        inputs = processor(images=image, return_tensors="pt")
        
        # Inference
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # L·∫•y top 3 predictions
        top3_indices = torch.topk(predictions, 3).indices[0]
        top3_scores = torch.topk(predictions, 3).values[0]
        
        # Format k·∫øt qu·∫£
        results = []
        for idx, score in zip(top3_indices, top3_scores):
            label = model.config.id2label[idx.item()]
            results.append({
                'label': label,
                'score': score.item()
            })
        
        return results
    except Exception as e:
        st.error(f"L·ªói khi ph√¢n t√≠ch ·∫£nh: {str(e)}")
        return None

def format_diagnosis_prompt(predictions):
    """T·∫°o prompt cho GPT-OSS t·ª´ k·∫øt qu·∫£ ch·∫©n ƒëo√°n"""
    prompt = "ƒê√¢y l√† ·∫£nh da li·ªÖu.\n\n"
    prompt += "Model th·ªã gi√°c d·ª± ƒëo√°n:\n"
    
    for i, pred in enumerate(predictions, 1):
        disease = pred['label']
        confidence = pred['score'] * 100
        prompt += f"{i}. {disease}, ƒë·ªô tin c·∫≠y {confidence:.1f}%\n"
    
    prompt += "\nGi·∫£i th√≠ch v√¨ sao model c√≥ th·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n n√†y d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm h√¨nh ·∫£nh. C≈©ng nh∆∞ gi·∫£i th√≠ch l√≠ do ƒë·ªô tin c·∫≠y ·ªü m·ª©c kia. L∆∞u √Ω: ƒê√¢y ch·ªâ l√† d·ª± ƒëo√°n c·ªßa AI, kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n y t·∫ø chuy√™n nghi·ªáp."
    
    return prompt

# Load model m·ªôt l·∫ßn
processor, model = load_skin_disease_model()

# Kh·ªüi t·∫°o session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "mode" not in st.session_state:
    st.session_state.mode = "chat"

# Header
st.title("ü©∫ Medical Chatbot")
st.markdown("---")

# Mode selection
col1, col2 = st.columns([3, 1])
with col1:
    st.markdown("### Ch·∫ø ƒë·ªô ho·∫°t ƒë·ªông:")
with col2:
    mode = st.selectbox(
        "Ch·ªçn ch·∫ø ƒë·ªô:",
        ["Chat th∆∞·ªùng", "Ch·∫©n ƒëo√°n da li·ªÖu"],
        index=0 if st.session_state.mode == "chat" else 1,
        key="mode_selector"
    )
    
    if mode == "Chat th∆∞·ªùng":
        st.session_state.mode = "chat"
    else:
        st.session_state.mode = "diagnosis"

st.markdown("---")

# Chat interface
chat_container = st.container()
with chat_container:
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user" and "image" in message:
                st.image(message["image"], width=300)
            st.markdown(message["content"])

# Input area
if st.session_state.mode == "chat":
    # Chat th∆∞·ªùng
    if prompt := st.chat_input("Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n..."):
        # Th√™m tin nh·∫Øn user
        st.session_state.messages.append({"role": "user", "content": prompt})
        
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # G·ªçi GPT-OSS
        with st.chat_message("assistant"):
            with st.spinner("ƒêang suy nghƒ©..."):
                messages_for_api = [{"role": msg["role"], "content": msg["content"]} 
                                  for msg in st.session_state.messages if "image" not in msg]
                response = call_gpt_oss(messages_for_api)
                st.markdown(response)
                st.session_state.messages.append({"role": "assistant", "content": response})

else:
    # Ch·∫ø ƒë·ªô ch·∫©n ƒëo√°n da li·ªÖu
    if processor is None or model is None:
        st.error("Model ch∆∞a ƒë∆∞·ª£c load th√†nh c√¥ng. Vui l√≤ng ki·ªÉm tra l·∫°i.")
    else:
        st.markdown("### üì∑ T·∫£i ·∫£nh da li·ªÖu ƒë·ªÉ ch·∫©n ƒëo√°n")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn ·∫£nh (JPG, JPEG, PNG):",
            type=['jpg', 'jpeg', 'png'],
            key="image_uploader"
        )
        
        if uploaded_file is not None:
            # Hi·ªÉn th·ªã ·∫£nh
            image = Image.open(uploaded_file)
            st.image(image, caption="·∫¢nh ƒë√£ t·∫£i l√™n", width=400)
            
            if st.button("üîç Ph√¢n t√≠ch ·∫£nh", type="primary"):
                with st.spinner("ƒêang ph√¢n t√≠ch ·∫£nh..."):
                    # Ph√¢n t√≠ch ·∫£nh v·ªõi model local
                    predictions = analyze_skin_image(image, processor, model)
                    
                    if predictions:
                        # Hi·ªÉn th·ªã ·∫£nh trong chat
                        st.session_state.messages.append({
                            "role": "user", 
                            "content": "T√¥i ƒë√£ g·ª≠i ·∫£nh da li·ªÖu ƒë·ªÉ ch·∫©n ƒëo√°n",
                            "image": image
                        })
                        
                        with st.chat_message("user"):
                            st.image(image, width=300)
                            st.markdown("T√¥i ƒë√£ g·ª≠i ·∫£nh da li·ªÖu ƒë·ªÉ ch·∫©n ƒëo√°n")
                        
                        # T·∫°o prompt cho GPT-OSS
                        diagnosis_prompt = format_diagnosis_prompt(predictions)
                        
                        # G·ªçi GPT-OSS v·ªõi prompt ch·∫©n ƒëo√°n
                        with st.chat_message("assistant"):
                            with st.spinner("ƒêang ph√¢n t√≠ch v√† gi·∫£i th√≠ch..."):
                                messages_for_api = [{"role": "user", "content": diagnosis_prompt}]
                                response = call_gpt_oss(messages_for_api)
                                st.markdown(response)
                                st.session_state.messages.append({"role": "assistant", "content": response})

# Sidebar v·ªõi th√¥ng tin
with st.sidebar:
    st.markdown("### ‚ÑπÔ∏è Th√¥ng tin")
    st.markdown("""
    **Ch·∫ø ƒë·ªô Chat th∆∞·ªùng:**
    - Chat v·ªõi AI nh∆∞ b√¨nh th∆∞·ªùng
    - S·ª≠ d·ª•ng GPT-OSS
    
    **Ch·∫ø ƒë·ªô Ch·∫©n ƒëo√°n:**
    - T·∫£i ·∫£nh da li·ªÖu l√™n
    - AI s·∫Ω ph√¢n t√≠ch v√† ƒë∆∞a ra d·ª± ƒëo√°n
    - Gi·∫£i th√≠ch chi ti·∫øt t·ª´ chuy√™n gia AI
    
    ‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:**
    K·∫øt qu·∫£ ch·ªâ mang t√≠nh tham kh·∫£o, 
    kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n y t·∫ø chuy√™n nghi·ªáp.
    """)
    
    if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat"):
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("**Model ƒëang s·ª≠ d·ª•ng:**")
    st.markdown("- Chat: GPT-OSS")
    st.markdown("- Vision: DinoV2 SkinDisease (Local)")
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i model
    if processor is not None and model is not None:
        st.success("‚úÖ Model ƒë√£ load th√†nh c√¥ng")
    else:
        st.error("‚ùå Model ch∆∞a load ƒë∆∞·ª£c")

# CSS t√πy ch·ªânh
st.markdown("""
<style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    
    .stChatMessage {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    
    .stButton > button {
        width: 100%;
        border-radius: 10px;
        border: none;
        background: linear-gradient(45deg, #4facfe 0%, #00f2fe 100%);
        color: white;
        font-weight: bold;
        padding: 0.5rem 1rem;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
    
    .stFileUploader {
        border: 2px dashed #4facfe;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)