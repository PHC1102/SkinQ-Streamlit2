import streamlit as st
import requests
import json
import base64
from PIL import Image
import io
import torch
from transformers import AutoImageProcessor, AutoModelForImageClassification
import numpy as np

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="Medical Chatbot",
    page_icon="ü©∫",
    layout="wide"
)

# L·∫•y API key t·ª´ secrets
OPENROUTER_API_KEY = st.secrets["OPENROUTER_API_KEY"]

# URLs API
OPENROUTER_URL = "https://openrouter.ai/api/v1/chat/completions"

@st.cache_resource
def load_skin_disease_model():
    """Load dinov2-skindisease-finetuned model locally"""
    try:
        processor = AutoImageProcessor.from_pretrained("Jayanth2002/dinov2-base-finetuned-SkinDisease")
        model = AutoModelForImageClassification.from_pretrained("Jayanth2002/dinov2-base-finetuned-SkinDisease")
        return processor, model
    except Exception as e:
        st.error(f"Kh√¥ng th·ªÉ load model: {str(e)}")
        return None, None

from openai import OpenAI

# T·∫°o client OpenAI nh∆∞ng tr·ªè sang OpenRouter
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=OPENROUTER_API_KEY,
)

def call_gpt_oss(messages):
    try:
        completion = client.chat.completions.create(
            model="openai/gpt-oss-20b:free",   # nh·ªõ ƒë√∫ng t√™n model
            messages=messages,
            temperature=0.7,
            max_tokens=1000,
            extra_headers={
                "HTTP-Referer": "https://skinq-app2-f7etnqs3jnubmrwnsjqdrq.streamlit.app",
                "X-Title": "SkinQ Chatbot",
            },
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"L·ªói khi g·ªçi API: {str(e)}"
        
def analyze_skin_image(image, processor, model):
    """Ph√¢n t√≠ch ·∫£nh da li·ªÖu b·∫±ng model local"""
    try:
        # Preprocess ·∫£nh
        inputs = processor(images=image, return_tensors="pt")
        
        # Inference
        with torch.no_grad():
            outputs = model(**inputs)
            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
        
        # L·∫•y top 3 predictions
        top3_indices = torch.topk(predictions, 3).indices[0]
        top3_scores = torch.topk(predictions, 3).values[0]
        
        # Format k·∫øt qu·∫£
        results = []
        for idx, score in zip(top3_indices, top3_scores):
            label = model.config.id2label[idx.item()]
            results.append({
                'label': label,
                'score': score.item()
            })
        
        return results
    except Exception as e:
        st.error(f"L·ªói khi ph√¢n t√≠ch ·∫£nh: {str(e)}")
        return None

def format_diagnosis_prompt(predictions):
    """T·∫°o prompt cho GPT-OSS t·ª´ k·∫øt qu·∫£ ch·∫©n ƒëo√°n"""
    prompt = "ƒê√¢y l√† ·∫£nh da li·ªÖu.\n\n"
    prompt += "Model th·ªã gi√°c d·ª± ƒëo√°n:\n"
    
    for i, pred in enumerate(predictions, 1):
        disease = pred['label']
        confidence = pred['score'] * 100
        prompt += f"{i}. {disease}, ƒë·ªô tin c·∫≠y {confidence:.1f}%\n"
    
    prompt += "\nGi·∫£i th√≠ch v√¨ sao model c√≥ th·ªÉ ƒë∆∞a ra d·ª± ƒëo√°n n√†y d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm h√¨nh ·∫£nh. L∆∞u √Ω: ƒê√¢y ch·ªâ l√† d·ª± ƒëo√°n c·ªßa AI, kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n y t·∫ø chuy√™n nghi·ªáp."
    
    return prompt

# Load model m·ªôt l·∫ßn
processor, model = load_skin_disease_model()

# Kh·ªüi t·∫°o session state
if "messages" not in st.session_state:
    st.session_state.messages = []
if "show_diagnosis" not in st.session_state:
    st.session_state.show_diagnosis = False

# Header
st.title("ü©∫ Medical Chatbot")
st.markdown("---")

# N√∫t ch·∫©n ƒëo√°n da li·ªÖu
col1, col2, col3 = st.columns([1, 2, 1])
with col2:
    if st.button("üî¨ Ch·∫©n ƒëo√°n b·ªánh da li·ªÖu", type="primary", use_container_width=True):
        st.session_state.show_diagnosis = True

st.markdown("---")

# Chat interface
chat_container = st.container()
with chat_container:
    # Hi·ªÉn th·ªã l·ªãch s·ª≠ chat
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            if message["role"] == "user" and "image" in message:
                st.image(message["image"], width=300)
            st.markdown(message["content"])

# Hi·ªÉn th·ªã upload ·∫£nh n·∫øu ƒë∆∞·ª£c k√≠ch ho·∫°t
if st.session_state.show_diagnosis:
    if processor is None or model is None:
        st.error("Model ch∆∞a ƒë∆∞·ª£c load th√†nh c√¥ng. Vui l√≤ng ki·ªÉm tra l·∫°i.")
        st.session_state.show_diagnosis = False
    else:
        st.markdown("### üì∑ Upload ·∫£nh da li·ªÖu ƒë·ªÉ ch·∫©n ƒëo√°n")
        
        uploaded_file = st.file_uploader(
            "Ch·ªçn ·∫£nh (JPG, JPEG, PNG):",
            type=['jpg', 'jpeg', 'png'],
            key="diagnosis_uploader"
        )
        
        # T·ª± ƒë·ªông ph√¢n t√≠ch khi c√≥ ·∫£nh upload
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
            
            # Hi·ªÉn th·ªã ·∫£nh ƒë√£ upload trong chat
            st.session_state.messages.append({
                "role": "user", 
                "content": "T√¥i ƒë√£ g·ª≠i ·∫£nh da li·ªÖu ƒë·ªÉ ch·∫©n ƒëo√°n",
                "image": image
            })
            
            with st.chat_message("user"):
                st.image(image, width=300)
                st.markdown("T√¥i ƒë√£ g·ª≠i ·∫£nh da li·ªÖu ƒë·ªÉ ch·∫©n ƒëo√°n")
            
            # T·ª± ƒë·ªông ph√¢n t√≠ch ngay
            with st.chat_message("assistant"):
                with st.spinner("ƒêang ph√¢n t√≠ch ·∫£nh..."):
                    # Ph√¢n t√≠ch ·∫£nh v·ªõi model local
                    predictions = analyze_skin_image(image, processor, model)
                    
                    if predictions:
                        # T·∫°o prompt cho GPT-OSS
                        diagnosis_prompt = format_diagnosis_prompt(predictions)
                        
                        with st.spinner("ƒêang t·∫°o b√°o c√°o ch·∫©n ƒëo√°n..."):
                            messages_for_api = [{"role": "user", "content": diagnosis_prompt}]
                            response = call_gpt_oss(messages_for_api)
                            st.markdown(response)
                            st.session_state.messages.append({"role": "assistant", "content": response})
                    else:
                        error_msg = "Kh√¥ng th·ªÉ ph√¢n t√≠ch ·∫£nh n√†y. Vui l√≤ng th·ª≠ ·∫£nh kh√°c."
                        st.error(error_msg)
                        st.session_state.messages.append({"role": "assistant", "content": error_msg})
            
            # Quay l·∫°i mode chat th∆∞·ªùng v√† reset
            st.session_state.show_diagnosis = False
            st.rerun()

# Chat input - lu√¥n hi·ªÉn th·ªã (m·∫∑c ƒë·ªãnh l√† chat th∆∞·ªùng)
if prompt := st.chat_input("Nh·∫≠p tin nh·∫Øn c·ªßa b·∫°n..."):
    # Th√™m tin nh·∫Øn user
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # G·ªçi GPT-OSS
    with st.chat_message("assistant"):
        with st.spinner("ƒêang suy nghƒ©..."):
            messages_for_api = [{"role": msg["role"], "content": msg["content"]} 
                              for msg in st.session_state.messages if "image" not in msg]
            response = call_gpt_oss(messages_for_api)
            st.markdown(response)
            st.session_state.messages.append({"role": "assistant", "content": response})

# Sidebar v·ªõi th√¥ng tin
with st.sidebar:
    st.markdown("### ‚ÑπÔ∏è Th√¥ng tin")
    st.markdown("""
    **üí¨ Chat th∆∞·ªùng:**
    - Lu√¥n s·∫µn s√†ng tr√≤ chuy·ªán
    - S·ª≠ d·ª•ng GPT-OSS
    
    **üî¨ Ch·∫©n ƒëo√°n da li·ªÖu:**
    - Nh·∫•n n√∫t "Ch·∫©n ƒëo√°n b·ªánh da li·ªÖu"
    - Upload ·∫£nh ‚Üí T·ª± ƒë·ªông ph√¢n t√≠ch
    - Quay l·∫°i chat th∆∞·ªùng sau khi xong
    
    ‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:**
    K·∫øt qu·∫£ ch·ªâ mang t√≠nh tham kh·∫£o, 
    kh√¥ng thay th·∫ø ch·∫©n ƒëo√°n y t·∫ø chuy√™n nghi·ªáp.
    """)
    
    if st.button("üóëÔ∏è X√≥a l·ªãch s·ª≠ chat"):
        st.session_state.messages = []
        st.rerun()
    
    st.markdown("---")
    st.markdown("**Model ƒëang s·ª≠ d·ª•ng:**")
    st.markdown("- Chat: GPT-OSS")
    st.markdown("- Vision: DinoV2 SkinDisease (Local)")
    
    # Hi·ªÉn th·ªã tr·∫°ng th√°i model
    if processor is not None and model is not None:
        st.success("‚úÖ Model ƒë√£ load th√†nh c√¥ng")
    else:
        st.error("‚ùå Model ch∆∞a load ƒë∆∞·ª£c")

# CSS t√πy ch·ªânh
st.markdown("""
<style>
    .stApp {
        max-width: 1200px;
        margin: 0 auto;
    }
    
    .stChatMessage {
        padding: 1rem;
        border-radius: 10px;
        margin-bottom: 1rem;
    }
    
    .stButton > button {
        width: 100%;
        border-radius: 10px;
        border: none;
        background: linear-gradient(45deg, #4facfe 0%, #00f2fe 100%);
        color: white;
        font-weight: bold;
        padding: 0.5rem 1rem;
        transition: all 0.3s ease;
    }
    
    .stButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 5px 15px rgba(0,0,0,0.2);
    }
    
    .stFileUploader {
        border: 2px dashed #4facfe;
        border-radius: 10px;
        padding: 2rem;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)
